# Multimodal and Multilingual NLI Dataset

This repository accompanies our Interspeech 2025 paper:  
**"Beyond Similarity Scoring: Detecting Entailment and Contradiction in Multilingual and Multimodal Contexts"**  
by *Othman Istaiteh, Salima Mdhaffar, Yannick Est√®ve*

## üìå Dataset Overview

This dataset supports multilingual and multimodal Natural Language Inference (NLI), enabling the classification of entailment, contradiction, and neutrality across four modality pairs: text-text, text-speech, speech-text, and speech-speech. It covers Arabic, English, and French, facilitating research beyond traditional similarity scoring methods by detecting logical relationships in both text and speech across multiple languages.

- **4 Modality Combinations**:
  - Text-Text (T-T)
  - Text-Speech (T-S)
  - Speech-Text (S-T) 
  - Speech-Speech (S-S)
  
- **Label System**:
  - `0` = Entailment
  - `1` = Contradiction  
  - `2` = Neutral

## üìÇ Dataset Directory Structure
```
dataset/
‚îú‚îÄ‚îÄ audio/
‚îÇ   ‚îú‚îÄ‚îÄ fleurs/
‚îÇ   ‚îî‚îÄ‚îÄ tts_generated/
‚îú‚îÄ‚îÄ dev.csv
‚îú‚îÄ‚îÄ test.csv
‚îú‚îÄ‚îÄ train_part_1.csv
‚îî‚îÄ‚îÄ train_part_2.csv

```
> **Note:** The original `train.csv` file has been split into two smaller files, `train_part_1.csv` and `train_part_2.csv`, to accommodate GitHub file size limits.

### üìù Data Composition

| Source Type          | Description                                                                 | Examples                     |
|----------------------|-----------------------------------------------------------------------------|------------------------------|
| **XNLI**             | Human-annotated text pairs (15 languages)                                   | Text-text pairs              |
| **SNLI**             | English image-caption derived NLI pairs                                     | Text-text pairs              |  
| **FLEURS**           | Natural speech recordings with transcriptions                               | Speech-text, speech-speech, text-text pairs            |
| **TTS_generated**    | Synthetic speech generated using the [Coqui TTS](https://github.com/coqui-ai/TTS) toolkit with a multispeaker VITS model (English only)                 | Speech-text, speech-speech   |
| **Mistral_generated** | NLI pairs generated by fine-tuned [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b) | Text-text, Speech-text pairs |

### üìä CSV File Specifications

Each CSV contains these columns:

| Column                  | Description                                                                 | Possible Values                              |
|-------------------------|-----------------------------------------------------------------------------|----------------------------------------------|
| `premise`               | Text content or audio path                                                  | String (text) or path (e.g. `audio/tts_generated/en_tts_dev_0.wav`) |
| `hypothesis`            | Text content or audio path                                                  | Same as premise                              |
| `label`                 | Relationship classification                                                 | `0` (entail), `1` (contradict), `2` (neutral) |
| `premise_modality`      | Data type of premise                                                        | `text` or `speech`                           |
| `hypothesis_modality`   | Data type of hypothesis                                                     | `text` or `speech`                           |
| `premise_language`      | Language of premise                                                         | `en`, `fr`, `ar`                             |
| `hypothesis_language`   | Language of hypothesis                                                      | `en`, `fr`, `ar`                             |
| `premise_source`        | Origin of premise data                                                      | `XNLI`, `SNLI`, `FLEURS`, `TTS_generated`, `Mistral_generated` |
| `hypothesis_source`     | Origin of hypothesis data                                                   | Same as premise_source                       |

## üì• Audio Data Download Instructions

1. Download audio components:
   - [FLEURS recordings (ZIP)](https://drive.google.com/file/d/1RdTbeLyYT6f7SzgEPMwRppX33L5GRfUa/view?usp=sharing)
   - [TTS-generated speech (ZIP)](https://drive.google.com/file/d/17h5LUJ7FFnoQOt8GETMpUUytPjOB_tRA/view?usp=sharing)
2. Move the downloaded ZIP files into the `dataset/audio/` directory.
3. Unzip them inside the `dataset/audio/` folder.
4. The training data has been split into two parts (`train_part_1.csv` and `train_part_2.csv`) due to file size limitations. Please ensure you use both parts for training.
5. You may need to replace the relative path `audio/` with the full absolute path for premise and hypothesis audio files in the CSVs.

## ‚öñÔ∏è License

This dataset combines:

- SNLI ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))  
- XNLI ([CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/))  
- FLEURS ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))  

The dataset as a whole is licensed under **CC BY-NC 4.0**, which is the most restrictive license among the components. It requires:

- Attribution  
- Non-commercial use  
- Share-alike (derivatives must use the same license)

## üìñ Citation

If you use this dataset, please cite our paper:

```bibtex
@inproceedings{istaiteh2025beyond,
  title={Beyond Similarity Scoring: Detecting Entailment and Contradiction in Multilingual and Multimodal Contexts},
  author={Istaiteh, Othman and Mdhaffar, Salima and Est√®ve, Yannick},
  booktitle={Interspeech 2025, Accepted Paper},
  year={2025}
}
```
## üìö References

- Jiang, A. Q. et al. (2023). *Mistral 7B*. arXiv preprint arXiv:2310.06825. [Link](https://arxiv.org/abs/2310.06825)  
- Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., Riesa, J., Rivera, C., & Bapna, A. (2022). *FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech*. arXiv preprint arXiv:2205.12446. [Link](https://arxiv.org/abs/2205.12446)  
- Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S. R., Schwenk, H., & Stoyanov, V. (2018). *XNLI: Evaluating Cross-lingual Sentence Representations*. EMNLP.  
- Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). *A large annotated corpus for learning natural language inference*. EMNLP.  
- G√∂lge, E., & The Coqui TTS Team (2021). *Coqui TTS: A deep learning toolkit for Text-to-Speech*. Zenodo. [DOI: 10.5281/zenodo.6334862](https://doi.org/10.5281/zenodo.6334862)

## üèõÔ∏è Affiliations

This work was conducted at [LIA Lab](https://lia.univ-avignon.fr/),  
part of [Universit√© d‚ÄôAvignon](https://univ-avignon.fr/).

## üì¨ Contact

For questions, suggestions, or collaboration inquiries, please contact:

**Othman Istaiteh**  
Email: othmanistaiteh@gmail.com

Feel free to open issues or pull requests on this repository.


